{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06acff78",
   "metadata": {},
   "source": [
    "# Q1 Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac32093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import branca.colormap as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c98c1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dev_wings_agg_span_2024_01_01.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m alert_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_wings_meteorology_station_summary_snapshot_2023_08_02.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m station_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgis_weatherstation_shape_2024_10_04.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m conductor_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdev_wings_agg_span_2024_01_01.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m vri_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_vri_snapshot_2024_03_20.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dev_wings_agg_span_2024_01_01.csv'"
     ]
    }
   ],
   "source": [
    "#load in datasets\n",
    "wind_data = pd.read_excel('src_wings_meteorology_windspeed_snapshot_2023_08_02.xlsx')\n",
    "alert_data = pd.read_excel('src_wings_meteorology_station_summary_snapshot_2023_08_02.xlsx')\n",
    "station_data = pd.read_excel('gis_weatherstation_shape_2024_10_04.xlsx')\n",
    "conductor_data = pd.read_csv('dev_wings_agg_span_2024_01_01.csv')\n",
    "vri_data = pd.read_csv('src_vri_snapshot_2024_03_20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11664d3b",
   "metadata": {},
   "source": [
    "# Task 1 EDA and Merging of Wind, Alert, Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates in wind_data\n",
    "cleaned_station_data = station_data.drop_duplicates(subset=['weatherstationcode'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdde56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebcaa8",
   "metadata": {},
   "source": [
    "## Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b064cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first merge station data and alert_data to get the alert data for each station\n",
    "station_alert_data = pd.merge(cleaned_station_data,alert_data[['station','vri','alert']], left_on = 'weatherstationcode', right_on = 'station').drop('station',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_station_alert_data = pd.merge(wind_data, station_alert_data, \n",
    "                     left_on='station', \n",
    "                     right_on='weatherstationcode', \n",
    "                     how='left')\n",
    "\n",
    "wind_station_alert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd192dd-7dd9-4ba0-9923-b71e93f8a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_speed_stats = wind_station_alert_data.groupby('station')['wind_speed'].agg(['mean', 'min', 'max'])\n",
    "wind_speed_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e965c",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf95082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical Wind Speed Distribution\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.histplot(data=wind_station_alert_data, x='wind_speed', kde=True, bins=30, color='skyblue')\n",
    "plt.title('Historical Wind Speed Distribution')\n",
    "plt.xlabel('Wind Speed (km/h)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a28977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of Alert Speeds\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=wind_station_alert_data, x='alert', bins=5, kde=True, color='skyblue')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Alert Speeds')\n",
    "plt.xlabel('Alert Speed (km/h)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adjust x-axis to show only the range of alert speeds\n",
    "plt.xlim(wind_station_alert_data['alert'].min() - 5, wind_station_alert_data['alert'].max() + 5)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffa873-9d61-4756-98e6-80053f2c8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_station_alert_data['geometry'] = wind_station_alert_data.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "wind_station_alert_data = gpd.GeoDataFrame(wind_station_alert_data, geometry='geometry')\n",
    "wind_station_alert_data.crs = 'EPSG:4326'\n",
    "\n",
    "wind_station_alert_data = wind_station_alert_data.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe93f44-f442-499d-9f2e-7647020aa7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' to datetime\n",
    "wind_station_alert_data['date'] = pd.to_datetime(wind_station_alert_data['date'])\n",
    "\n",
    "for station in wind_station_alert_data['station'].unique():\n",
    "    station_data = wind_station_alert_data[wind_station_alert_data['station'] == station]\n",
    "    plt.figure()\n",
    "    plt.plot(station_data['date'], station_data['wind_speed'], label=f\"Wind Speed at {station}\")\n",
    "    plt.title(f\"Wind Speed Over Time at {station}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Wind Speed\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc3e9a-64a3-46eb-896b-0eb779042309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base map\n",
    "m = folium.Map(location=[gdf_weatherstation.geometry.y.mean(), gdf_weatherstation.geometry.x.mean()], zoom_start=10)\n",
    "\n",
    "# Add markers for each station with a popup showing key metrics\n",
    "for _, row in gdf_weatherstation_merged.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=f\"Station: {row['station']}, Wind Speed: {row['wind_speed']}, Max Gust: {row['max_gust']}\"\n",
    "    ).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20612b",
   "metadata": {},
   "source": [
    "# Task 2 Calculating PSPS Probability of Weather Stations and displaying results\n",
    "- 1. Calculating PSPS Probability for each Weather Station \n",
    "- 2. Visualizing Weather Station wind speed distributions against its wind speed threshold, and displaying its calculated PSPS Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c822fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_psps_prob_data = (\n",
    "    wind_station_alert_data.assign(exceeds_alert=lambda x: (x['wind_speed'] > x['alert']))  # Create a boolean column\n",
    "      .groupby('weatherstationcode')\n",
    "      .agg(total_records=('exceeds_alert', 'size'),  # Total records per station\n",
    "           exceeds_count=('exceeds_alert', 'sum'))   # Count where wind_speed > alert\n",
    "      .assign(percentage_of_PSPS_shutdown=lambda  x: (x['exceeds_count'] / x['total_records']))\n",
    "      [['percentage_of_PSPS_shutdown']]  # Select only the relevant column\n",
    ")\n",
    "\n",
    "weather_station_psps_prob_data.sort_values(by = 'percentage_of_PSPS_shutdown', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(wind_station_alert_data, weather_station_psps_prob_data, on='weatherstationcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6983405",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = merged_df.groupby('station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c73fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "stations_per_batch = 9  # Number of stations per figure (e.g., a grid of 3x3 subplots)\n",
    "num_stations = len(grouped)\n",
    "num_batches = (num_stations // stations_per_batch) + (num_stations % stations_per_batch > 0)\n",
    "\n",
    "# Loop through batches\n",
    "for batch in range(num_batches):\n",
    "    start_idx = batch * stations_per_batch\n",
    "    end_idx = min(start_idx + stations_per_batch, num_stations)\n",
    "    \n",
    "    # Create subplots for this batch\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=3,\n",
    "        ncols=3,\n",
    "        figsize=(15, 12),\n",
    "        sharey=True,\n",
    "        tight_layout=True\n",
    "    )\n",
    "    axes = axes.flatten()  # Flatten axes for easier iteration\n",
    "    \n",
    "    for ax_idx, (station_name, group) in enumerate(list(grouped)[start_idx:end_idx]):\n",
    "        ax = axes[ax_idx]\n",
    "        \n",
    "        # Plot histogram of wind speeds\n",
    "        ax.hist(group['wind_speed'], bins=10, alpha=0.7, color='skyblue', edgecolor='black', label='Wind Speed')\n",
    "        \n",
    "        # Add vertical line for wind speed threshold\n",
    "        threshold = group['alert'].iloc[0]\n",
    "        ax.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label='Threshold')\n",
    "        \n",
    "        # Add title with PSPS probability\n",
    "        psps_probability = group['percentage_of_PSPS_shutdown'].iloc[0]\n",
    "        ax.set_title(f'{station_name}\\nPSPS Probability: {psps_probability:.2f}')\n",
    "        \n",
    "        # Add labels and legend\n",
    "        ax.set_xlabel('Wind Speed (m/s)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend()\n",
    "    \n",
    "    # Hide unused subplots in the grid\n",
    "    for ax in axes[len(list(grouped)[start_idx:end_idx]):]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Save or show the plot for this batch\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'weather_station_batch_{batch + 1}.png')  # Save each batch as a separate file\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588dd40",
   "metadata": {},
   "source": [
    "# Task 3 Merging weather station data to conductor spans and displaying PSPS Probabilities across all 3 layers geospatially\n",
    "- 1) Merge weather stations to VRI polygons\n",
    "- 2) Merge VRI polgyons to Conductor Spans\n",
    "- 3) Displaying PSPS Probabilities across all 3 layers geospatially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00953920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging weather station + alert data and vri data\n",
    "station_vri_data = station_alert_data.merge(vri_data, left_on=\"weatherstationcode\", right_on=\"anemometercode\", how=\"inner\")\n",
    "station_vri_data = pd.merge(station_vri_data, weather_station_psps_prob_data, on='weatherstationcode')\n",
    "def rename_columns(col):\n",
    "    if col.endswith('_x'):\n",
    "        return col[:-2] + '_station'\n",
    "    elif col.endswith('_y'):\n",
    "        return col[:-2] + '_vri'\n",
    "    return col\n",
    "\n",
    "# Rename the columns\n",
    "station_vri_data = station_vri_data.rename(columns=rename_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_vri_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d94b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather station and vri data + conductor span data geospatial span\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "\n",
    "\n",
    "# Convert WKT strings to geometry objects for conductor data\n",
    "conductor_data['geometry'] = conductor_data['shape'].apply(wkt.loads)\n",
    "\n",
    "# Create GeoDataFrame from conductor data\n",
    "gdf1 = gpd.GeoDataFrame(conductor_data, geometry='geometry', crs=f\"EPSG:{conductor_data['shape_srid'].iloc[0]}\")\n",
    "\n",
    "# convert WKT string to geometry objects for conductor data\n",
    "station_vri_data['geometry'] = station_vri_data['shape_vri'].apply(wkt.loads)\n",
    "gdf2 = gpd.GeoDataFrame(station_vri_data, geometry='geometry', crs=f\"EPSG:{station_vri_data['shape_srid_vri'].iloc[0]}\")\n",
    "\n",
    "# Ensure both GeoDataFrames have the same CRS\n",
    "if gdf1.crs != gdf2.crs:\n",
    "    gdf2 = gdf2.to_crs(gdf1.crs)\n",
    "\n",
    "# Perform the spatial join\n",
    "joined_gdf = gpd.sjoin(gdf1, gdf2, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# Post-processing steps\n",
    "# Rename columns from VRI data to avoid confusion\n",
    "columns_to_rename = [col for col in joined_gdf.columns if col in gdf2.columns and col != 'geometry']\n",
    "joined_gdf = joined_gdf.rename(columns={col: f\"{col}_vri\" for col in columns_to_rename})\n",
    "joined_gdf.columns = joined_gdf.columns.str.replace('vri_vri', 'vri')\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['index_right'] \n",
    "joined_gdf = joined_gdf.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Reset index if needed\n",
    "joined_gdf = joined_gdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c234ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf = pd.merge(joined_gdf, weather_station_psps_prob_data, left_on='weatherstationcode_vri', right_on ='weatherstationcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa06b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_vri_data[['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting weather stations and psps probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_psps_data = pd.merge(cleaned_station_data,weather_station_psps_prob_data, on = \"weatherstationcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5bec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_lat = station_psps_data['latitude'].mean()\n",
    "center_lon = station_psps_data['longitude'].mean()\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, station in station_psps_data.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[station['latitude'], station['longitude']],\n",
    "        popup=f\"Station: {station['weatherstationname']}<br>PSPS Probability: {station['percentage_of_PSPS_shutdown']:.2f}\",\n",
    "        tooltip=station['weatherstationname']\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map\n",
    "m.save(\"weather_stations_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vri_data_psps_prob = pd.merge(vri_data, weather_station_psps_prob_data, right_on='weatherstationcode',left_on = \"anemometercode\")\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(vri_data_psps_prob, geometry=gpd.GeoSeries.from_wkt(vri_data_psps_prob['shape']))\n",
    "gdf.set_crs(epsg=4326, inplace=True)\n",
    "m = folium.Map(location=[gdf.geometry.centroid.y.mean(), gdf.geometry.centroid.x.mean()], zoom_start=6)\n",
    "\n",
    "# Function to style each featureSW\n",
    "def style_function(feature):\n",
    "    return {\n",
    "        'fillColor': '#ffaf00',\n",
    "        'color': 'black',\n",
    "        'weight': 2,\n",
    "        'fillOpacity': 0.7,\n",
    "    }\n",
    "\n",
    "# Add GeoJson layer\n",
    "folium.GeoJson(\n",
    "    gdf,\n",
    "    style_function=style_function,\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['name', 'percentage_of_PSPS_shutdown', 'hftd'], aliases=['Name', 'Percentage_PSPS_Shutdown', 'HFTD']),\n",
    "    popup=folium.GeoJsonPopup(fields=['name', 'vri_risk', 'hftd', 'gust_max'], aliases=['Name', 'VRI Risk', 'HFTD', 'Max Gust'])\n",
    ").add_to(m)\n",
    "\n",
    "# Save the map\n",
    "m.save(\"vri_shapes_pspsprob.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ba0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = cm.LinearColormap(\n",
    "    colors=['green', 'yellow', 'red'],\n",
    "    vmin=gdf['percentage_of_PSPS_shutdown'].min(),\n",
    "    vmax=gdf['percentage_of_PSPS_shutdown'].max(),\n",
    "    caption='PSPS Shutdown Probability'\n",
    ")\n",
    "\n",
    "# Modify the style function to use the colormap\n",
    "def style_function(feature):\n",
    "    psps_value = feature['properties']['percentage_of_PSPS_shutdown']\n",
    "    return {\n",
    "        'fillColor': colormap(psps_value),\n",
    "        'color': 'black',\n",
    "        'weight': 2,\n",
    "        'fillOpacity': 0.7,\n",
    "    }\n",
    "\n",
    "# Create the map\n",
    "m = folium.Map(location=[gdf.geometry.centroid.y.mean(), gdf.geometry.centroid.x.mean()], \n",
    "               zoom_start=6)\n",
    "\n",
    "# Add GeoJson layer with the new style function\n",
    "folium.GeoJson(\n",
    "    gdf,\n",
    "    style_function=style_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=['name', 'percentage_of_PSPS_shutdown', 'hftd'], \n",
    "        aliases=['Name', 'Percentage_PSPS_Shutdown', 'HFTD']\n",
    "    ),\n",
    "    popup=folium.GeoJsonPopup(\n",
    "        fields=['name', 'vri_risk', 'hftd', 'gust_max'], \n",
    "        aliases=['Name', 'VRI Risk', 'HFTD', 'Max Gust']\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Add the colormap to the map\n",
    "colormap.add_to(m)\n",
    "\n",
    "# Save the map\n",
    "m.save(\"vri_shapes_pspsprob2.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5389191",
   "metadata": {},
   "source": [
    "# Task 4 Creating graph network object of spans to trace upstream/downtream the grid; collect list of weather stations that could cause a shut-off to any given span "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6329973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate station data\n",
    "station_data = station_data.drop_duplicates(subset=['weatherstationcode'], keep='first')\n",
    "#merge alert data to station data so for each station have alert threshold\n",
    "alert_station_data = pd.merge(station_data,alert_data[['station','vri','alert']], left_on = 'weatherstationcode', right_on = 'station').drop('station',axis = 1)\n",
    "\n",
    "station_vri_data = alert_station_data.merge(vri_data, left_on=\"weatherstationcode\", right_on=\"anemometercode\", how=\"inner\")\n",
    "\n",
    "# Define a function to rename columns\n",
    "def rename_columns(col):\n",
    "    if col.endswith('_x'):\n",
    "        return col[:-2] + '_station'\n",
    "    elif col.endswith('_y'):\n",
    "        return col[:-2] + '_vri'\n",
    "    return col\n",
    "\n",
    "# Rename the columns\n",
    "station_vri_data = station_vri_data.rename(columns=rename_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather station and vri data + conductor span data geospatial span\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "\n",
    "# Assuming df1 is your Conductor span DataFrame and df2 is your VRI polygon DataFrame\n",
    "\n",
    "# Convert WKT strings to geometry objects for df1\n",
    "conductor_data['geometry'] = conductor_data['shape'].apply(wkt.loads)\n",
    "\n",
    "# Create GeoDataFrame from df1\n",
    "gdf1 = gpd.GeoDataFrame(conductor_data, geometry='geometry', crs=f\"EPSG:{conductor_data['shape_srid'].iloc[0]}\")\n",
    "\n",
    "# Do the same for df2 if necessary\n",
    "station_vri_data['geometry'] = station_vri_data['shape_vri'].apply(wkt.loads)\n",
    "gdf2 = gpd.GeoDataFrame(station_vri_data, geometry='geometry', crs=f\"EPSG:{station_vri_data['shape_srid_vri'].iloc[0]}\")\n",
    "\n",
    "# Ensure both GeoDataFrames have the same CRS\n",
    "if gdf1.crs != gdf2.crs:\n",
    "    gdf2 = gdf2.to_crs(gdf1.crs)\n",
    "\n",
    "# Perform the spatial join\n",
    "joined_gdf = gpd.sjoin(gdf1, gdf2, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# Post-processing steps\n",
    "# Rename columns from VRI data to avoid confusion\n",
    "columns_to_rename = [col for col in joined_gdf.columns if col in gdf2.columns and col != 'geometry']\n",
    "joined_gdf = joined_gdf.rename(columns={col: f\"{col}_vri\" for col in columns_to_rename})\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['index_right']  # Add any other columns you don't need\n",
    "joined_gdf = joined_gdf.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Reset index if needed\n",
    "joined_gdf = joined_gdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac8549",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "for _, row in conductor_data.iterrows():\n",
    "    if pd.notnull(row['upstream_span_id']):\n",
    "        upstream_id = row['upstream_span_id']\n",
    "        current_id = row['globalid']\n",
    "        G.add_edge(upstream_id, current_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_joined_gdf = joined_gdf.merge(weather_station_psps_prob_data, left_on = 'station',right_on = \"weatherstationcode\",how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae262b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_joined_gdf = new_joined_gdf[(new_joined_gdf['percentage_of_PSPS_shutdown'] > 0)]\n",
    "new_joined_gdf = new_joined_gdf[['globalid','upstream_span_id', 'weatherstationcode_vri','parent_feederid']].drop_duplicates(subset = ['upstream_span_id','weatherstationcode_vri'])\n",
    "new_joined_gdf.dropna(inplace = True)\n",
    "#new_joined_gdf.dropna(inplace = True, subset = ['upstream_span_id', 'station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87cc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_feederid_conductor_df= {group: data for group, data in new_joined_gdf.groupby('parent_feederid')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_spans(G, span_id):\n",
    "    \"\"\"Get all upstream and downstream spans of a given span.\"\"\"\n",
    "    # downstream_spans = list(nx.descendants(G, span_id))\n",
    "    upstream_spans = list(nx.ancestors(G, span_id))\n",
    "    return upstream_spans\n",
    "\n",
    "def get_weather_station_associations(impacted_spans):\n",
    "    unique_spans = impacted_spans['upstream_span_id'].unique()\n",
    "    span_weather_stations = {}\n",
    "\n",
    "    for span_id in unique_spans:\n",
    "        traced_spans = trace_spans(G, span_id)\n",
    "        \n",
    "        traced_spans.append(span_id)\n",
    "        \n",
    "        associated_stations = impacted_spans[\n",
    "            impacted_spans['upstream_span_id'].isin(traced_spans)\n",
    "        ]['weatherstationcode_vri'].unique()\n",
    "        \n",
    "        span_weather_stations[span_id] = list(associated_stations)\n",
    "\n",
    "    output_df = pd.DataFrame(\n",
    "        [(span, stations) for span, stations in span_weather_stations.items()],\n",
    "        columns=['upstream_span_id', 'associated_station']\n",
    "    )\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f6c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: For each group, create a directed graph and trace upstream spans\n",
    "def process_group(group_data):\n",
    "    G = nx.DiGraph()\n",
    "    # Add edges based on upstream and downstream structure IDs\n",
    "    for _, row in group_data.iterrows():\n",
    "        if pd.notnull(row['upstream_span_id']):\n",
    "            upstream_id = row['upstream_span_id']\n",
    "            current_id = row['globalid']\n",
    "            G.add_edge(upstream_id, current_id)\n",
    "\n",
    "    # Apply the function to populate the new column with upstream spans\n",
    "    \n",
    "    return get_weather_station_associations(group_data)\n",
    "\n",
    "# Step 3: Apply the process to each group and concatenate the results\n",
    "processed_groups = [process_group(group_data) for group_data in grouped_feederid_conductor_df.values()]\n",
    "final_df = pd.concat(processed_groups)\n",
    "\n",
    "# Step 4: Display the final DataFrame with the new column\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df['upstream_span_id'] == \"{85FA1DBE-DDAE-43D8-BBFA-CFD8C0D65678}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41428e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "span_station_exploded = final_df.explode('associated_station').rename(columns={'associated_station': 'station'})\n",
    "\n",
    "merged = span_station_exploded.merge(weather_station_psps_prob_data, left_on='station', right_on =\"weatherstationcode\",how='left')\n",
    "\n",
    "def calculate_union_probability(group):\n",
    "    \"\"\"Calculate union probability for a span's associated stations.\"\"\"\n",
    "    probs = group['percentage_of_PSPS_shutdown'].dropna()\n",
    "    if len(probs) == 0:\n",
    "        return np.nan\n",
    "    union_prob = 1 - np.prod(1 - probs)\n",
    "    return union_prob\n",
    "\n",
    "psps_probabilities = merged.groupby('upstream_span_id').apply(calculate_union_probability).reset_index()\n",
    "psps_probabilities.columns = ['span_id', 'psps_probability']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11809ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "psps_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58623eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "psps_probabilities[psps_probabilities['span_id'] == \"{FFF48154-5787-48BA-87CF-7C07CDAA3C75}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e0686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf[joined_gdf['upstream_span_id'] == \"{0000602C-82D9-445A-B1AF-81BDFB0EEC14}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b9b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf_psps_probs = joined_gdf.merge(psps_probabilities,left_on = 'upstream_span_id',right_on =\"span_id\",how = \"right\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf_psps_probs[['upstream_span_id',\"span_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf_psps_probs['upstream_span_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00009a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de4082",
   "metadata": {},
   "source": [
    "# Task 5 Computing PSPS Probability of every conductor span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ba14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "psps_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d5e41",
   "metadata": {},
   "source": [
    "# Task 6 Estimate expected customers that'll be shut-off over the next 10 years at the span/segment/circuit granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09893ca3",
   "metadata": {},
   "source": [
    "### Expected Value approach\n",
    "Say for example your Probability is 5% on any single high fire day captured in the station data. And then you can calculate the  average days per year that the station data has collected during these high fire days. Assume it's  13 days per year. Then 13 x 0.05 = 0.65 can be assessed as the expected annual rate of shut-offs for that span. And then 0.65 * 10 = 6.5 shut-offs expected over 10 years for the customers associated to that span. And then you could aggregated that to the feederid level.\n",
    "That is one way to do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def228f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_data['wind_speed'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_data_cleaned = wind_data.dropna(subset = ['wind_speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2387a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_data_cleaned.groupby('station').size().unique() #each station has 179 entires "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2adf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime and extract year\n",
    "wind_data_cleaned['year'] = pd.to_datetime(wind_data_cleaned['date']).dt.year\n",
    "\n",
    "# Group by year and station\n",
    "grouped_df = wind_data_cleaned.groupby(['year', 'station']).size().reset_index(name = \"Annual_high_fire_days\")\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df[grouped_df['station'] == 'AMO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_high_fire_days_per_station = grouped_df.groupby('station')['Annual_high_fire_days'].mean().reset_index(name='Average_Annual_high_fire_days')\n",
    "average_high_fire_days_per_station.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3bb9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_psps_prob_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of weather station psps shutdown percentage\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=weather_station_psps_prob_data['percentage_of_PSPS_shutdown'], fill=True)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Distribution of PSPS Shutdown Probabilities')\n",
    "plt.xlabel('Probability of PSPS Shutdown')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Add a rug plot to show the actual data points\n",
    "sns.rugplot(data=weather_station_psps_prob_data['percentage_of_PSPS_shutdown'], color='red', height=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40445dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=weather_station_psps_prob_data, x='percentage_of_PSPS_shutdown', bins=20, kde=True)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Distribution of PSPS Shutdown Probabilities', fontsize=16)\n",
    "plt.xlabel('Probability of PSPS Shutdown', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Add a vertical line for the mean\n",
    "mean_probability = weather_station_psps_prob_data['percentage_of_PSPS_shutdown'].mean()\n",
    "plt.axvline(mean_probability, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {mean_probability:.4f}')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(average_high_fire_days_per_station,weather_station_psps_prob_data, left_on = \"station\", right_on = \"weatherstationcode\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca89451",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['station','Average_Annual_high_fire_days','percentage_of_PSPS_shutdown']]\n",
    "test['Annual_number_of_shutoffs_10years'] = (test['Average_Annual_high_fire_days'] * test['percentage_of_PSPS_shutdown'] * 10)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutoff_rates_dict = test.set_index('station')['Annual_number_of_shutoffs_10years'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735dc93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = joined_gdf_psps_probs.merge(final_df,on = \"upstream_span_id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_rate_shutoff_dict = test.set_index('station')['Annual_number_of_shutoffs_10years'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_rate_shutoff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee260f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weather_station_annual_shutoff_rates(df, shutoff_rates_dict):\n",
    "    def get_annual_shutoff_rates(station_codes):\n",
    "        probabilities = [shutoff_rates_dict.get(code, 0) for code in station_codes]\n",
    "        return probabilities\n",
    "    \n",
    "    def calculate_total_shutoffs(shutoffs):\n",
    "        if not shutoffs:\n",
    "            return 0\n",
    "        shutoffs = [x for x in shutoffs if x != 0]\n",
    "        total_prob = np.sum(shutoffs)\n",
    "        return total_prob\n",
    "    \n",
    "    df['annual_shutoff_counts_10years'] = df['associated_station'].apply(get_annual_shutoff_rates)\n",
    "    df['total_number_shutoff_10years'] = df['annual_shutoff_counts_10years'].apply(calculate_total_shutoffs)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = process_weather_station_annual_shutoff_rates(test2,shutoff_rates_dict)\n",
    "output['total_number_customers_affected_ten_years'] = output['total_number_shutoff_10years'] * (output['downstream_cust_total'] + output['cust_total'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1258ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[['globalid','associated_station','upstream_struct_id','downstream_struct_id','downstream_cust_total','cust_total','annual_shutoff_counts_10years','total_number_shutoff_10years']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution plot of total number of shutoffs\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=output, x='total_number_shutoff_10years', bins=50)\n",
    "plt.title('Distribution of Total Number of Shutoffs Over 10 Years (Span Level)')\n",
    "plt.xlabel('Number of Shutoffs')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Create boxplot to show the distribution of shutoffs by number of associated stations\n",
    "plt.figure(figsize=(12, 6))\n",
    "output['num_stations'] = output['associated_station'].str.len()\n",
    "sns.boxplot(x='num_stations', y='total_number_shutoff_10years', data=output)\n",
    "plt.title('Distribution of Shutoffs by Number of Associated Stations')\n",
    "plt.xlabel('Number of Associated Stations')\n",
    "plt.ylabel('Total Number of Shutoffs (10 Years)')\n",
    "\n",
    "# Create scatter plot of customers affected vs shutoffs\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=output, \n",
    "                x='total_number_shutoff_10years', \n",
    "                y='cust_total',\n",
    "                alpha=0.5)\n",
    "plt.title('Relationship Between Number of Shutoffs and Customers Affected')\n",
    "plt.xlabel('Total Number of Shutoffs (10 Years)')\n",
    "plt.ylabel('Number of Customers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d2d35",
   "metadata": {},
   "source": [
    "# Task 6 Estimate expected customers that'll be shut-off over the next 10 years at the span/segment/circuit granularity\n",
    "#### segment level = upstreamardfacilityid\n",
    "#### circuit = parent_feederid\n",
    "#### span = globalid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc1343",
   "metadata": {},
   "source": [
    "# Circuit Level (feederid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_feederid_df = output.groupby('feederid', as_index=False).agg(\n",
    "    total_number_shutoffs_feederid = ('total_number_shutoff_10years','sum'),\n",
    "    total_customers = ('cust_total','sum'),\n",
    "    total_downstream_customers = ('downstream_cust_total','sum'),\n",
    "    total_number_affected_customers_10years = ('total_number_customers_affected_ten_years','sum')\n",
    ")\n",
    "aggregated_feederid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=aggregated_feederid_df, x='total_number_affected_customers_10years', bins=50)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Distribution of Total Number of Customers Affected Over 10 Years (Span Level)', fontsize=14)\n",
    "plt.xlabel('Number of Customers Affected', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c912f7",
   "metadata": {},
   "source": [
    "# Span Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_out = output[['globalid','total_number_shutoff_10years','cust_total','downstream_cust_total','total_number_customers_affected_ten_years']]\n",
    "span_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a080d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conductor_data[conductor_data['globalid'] == \"{E335ABFD-E9B0-497C-A182-837D25ED6FE4}\"]['cust_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e5d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=span_out, x='total_number_customers_affected_ten_years', bins=10)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Distribution of Total Number of Customers Affected Over 10 Years (Span Level)', fontsize=14)\n",
    "plt.xlabel('Number of Customers Affected', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19730fb9",
   "metadata": {},
   "source": [
    "# Segment Level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba784b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_feederid_df = output.groupby('upstreamardfacilityid', as_index=False).agg(\n",
    "    total_number_shutoffs_feederid = ('total_number_shutoff_10years','sum'),\n",
    "    total_customers = ('cust_total','sum'),\n",
    "    total_downstream_customers = ('downstream_cust_total','sum'),\n",
    "    total_number_affected_customers_10years = ('total_number_customers_affected_ten_years','sum')\n",
    ")\n",
    "aggregated_feederid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e2497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
